{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NabZ2lZGEUaS"
      },
      "source": [
        "# CMPE 252, Section 01, HW2 - FALL 2023\n",
        "\n",
        "\n",
        "## Topic: Sequential decision making with discrete state and action spaces.\n",
        "\n",
        "In this assignment, you will solve MDP by **Value Iteration** and **Policy Iteration**\n",
        "algorithms with a known dynamics and reward, $T$ and $R$, respectively.\n",
        "\n",
        "The assignment has 2 parts. In Part-1, you are asked to fill in the blanks in the given code, and to complete the given tasks. In Part-2, you are asked to implement the Policy Iteration algorithm using tensor broadcasting (no for loops).\n",
        "\n",
        "You can work in teams of 2 students. Please assign yourselves to the pair in Canvas.\n",
        "\n",
        "You can discuss your solutions with other teams, but sharing your code or parts of it with other teams is plagiarism.\n",
        "\n",
        "What to submit in Canvas: 1) a working notebook with the full solution, and 2) its corresponding PDF.\n",
        "\n",
        "\n",
        "Due date : October 10, 2023, 11:59PM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnzJ5wJKmv5H"
      },
      "source": [
        "# <h1><center> Part-1 </center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "lm_n1eT3EUaU"
      },
      "source": [
        "#### Import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "XeIvwWOcEUaV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "import datetime\n",
        "import random\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJombMWUmv5L"
      },
      "source": [
        "<font color='red'>\n",
        "You can use your maze from HW1. Make sure that there are multiple paths (at least 3) from the START = 'top left corner' to GOAL='bottom right corner'. If you do not have at least 3 paths, update your maze accordingly\n",
        "</font>\n",
        "\n",
        "#### Design a maze:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "gzMMg9NpQ5dD"
      },
      "outputs": [],
      "source": [
        "def get_maze(maze_file):\n",
        "    '''\n",
        "    para1: filename of the maze txt file\n",
        "    return mazes as a numpy array walls: 0 - no wall, 1 - wall\n",
        "    '''\n",
        "    a = open(maze_file, 'r')\n",
        "    m=[]\n",
        "    for i in a.readlines():\n",
        "        m.append(np.array(i.split(\" \"), dtype=\"int32\"))\n",
        "    return np.array(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "jLFgpnM_EUaW"
      },
      "outputs": [],
      "source": [
        "# State Space\n",
        "S=get_maze(\"maze_25x25.txt\")\n",
        "START = (1,1)\n",
        "GOAL = (24,24)\n",
        "# Action Space\n",
        "A = [\n",
        "    (-1, 0),    # 'up'\n",
        "    (1, 0),     # 'down'\n",
        "    (0, -1),    # 'left'\n",
        "    (0, 1),     # 'right'\n",
        "    (0, 0)      # 'stay'\n",
        "]\n",
        "\n",
        "\n",
        "# Noise\n",
        "ALPHA = [0.2, 0.8]\n",
        "\n",
        "max_it = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "LoVFsScnEUaW"
      },
      "outputs": [],
      "source": [
        "GRID_SIZE = len(S)\n",
        "# goal state\n",
        "S[GOAL] = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8wrecEQEUaX"
      },
      "source": [
        "#### Visualize the maze:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TDXiKR5eEUaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78432905-b741-4850-c165-20a9e841fdd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1]\n",
            " [1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1]\n",
            " [1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1]\n",
            " [1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1]\n",
            " [1 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1]\n",
            " [1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1]\n",
            " [1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1]\n",
            " [1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1]\n",
            " [1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1]\n",
            " [1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1]\n",
            " [1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1]\n",
            " [1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1]\n",
            " [1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1]\n",
            " [1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1]\n",
            " [1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1]\n",
            " [1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1]\n",
            " [1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1]\n",
            " [1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1]\n",
            " [1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1]\n",
            " [1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1]\n",
            " [1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1]\n",
            " [1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3]]\n"
          ]
        }
      ],
      "source": [
        "print(S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "4R_xMFadEUaY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "826b70ed-3226-4960-88bf-1007cae3c8ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a4646237d30>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(S, cmap='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD7ZHnSZEUaY"
      },
      "source": [
        "#### Define a utility function, s_next_calc, which computes the index of the next state given current state and action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "bFRfCFu1EUaZ"
      },
      "outputs": [],
      "source": [
        "def s_next_calc(s, a):\n",
        "    '''This function returns the agent's next state given action\n",
        "    and current state (assuming the action succeeds).\n",
        "    : param s: Current position of the agent\n",
        "    : param a: action taken by the agent\n",
        "    : returns: New state coordinates in the grid\n",
        "    '''\n",
        "\n",
        "    return (s[0] + A[a][0], s[1] + A[a][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um83WW0EEUaZ"
      },
      "source": [
        "#### Define a utility function to check if the action at current state leads to a collision with a wall:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "gGOTonn_EUaZ"
      },
      "outputs": [],
      "source": [
        "def hit_wall(curr, action):\n",
        "    '''This function checks if the agent hits any walls.\n",
        "    : param curr: Current position of the agent\n",
        "    : param action: Chosen action by the agent\n",
        "    : returns: True/False Binary value to indicate if agent hits a wall\n",
        "    '''\n",
        "    s_new = (\n",
        "        curr[0] + A[action][0],\n",
        "        curr[1] + A[action][1]\n",
        "    )\n",
        "\n",
        "    # Check for grid boundaries\n",
        "    if min(s_new) < 0 or max(s_new) > GRID_SIZE-1:\n",
        "        return True\n",
        "\n",
        "    # Check walls\n",
        "    # 0: 'up':   (-1,  0),\n",
        "    # 1: 'down': ( 1,  0),\n",
        "    # 2: 'left': ( 0, -1),\n",
        "    # 3: 'right':( 0,  1),\n",
        "    # 4: 'stay': ( 0,  0)\n",
        "    if (S[curr]==0 and S[s_new]==1):\n",
        "        return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieW0NgQmEUaa"
      },
      "source": [
        "#### Define the reward function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "9dZ8GZkiEUaa"
      },
      "outputs": [],
      "source": [
        "def R(s, a):\n",
        "    '''Reward function\n",
        "    : param s: Current state of the agent\n",
        "    : param a: Action the agent takes at the current state\n",
        "    : returns: Reward for the action at current state\n",
        "    '''\n",
        "    if s == GOAL:\n",
        "        return 0\n",
        "    elif hit_wall(s, a):\n",
        "        return -10000\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knmSD8o_EUaa"
      },
      "source": [
        "#### Calculate the transition probabilities to state s_next from current state s upon action a:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "c3wdRpJWEUaa"
      },
      "outputs": [],
      "source": [
        "def Pr(s_next, s, a, alpha):\n",
        "    '''This function returns probabilities for each action at agent's\n",
        "    current state.\n",
        "    :param s: Current state of the agent\n",
        "    :param a: Action the agent takes at the current state\n",
        "    :param alpha: Probability of the agent to take a random action instead of the action a\n",
        "    :returns : Transition probability for the action at current state\n",
        "    '''\n",
        "    # can't move once reached the GOAL\n",
        "    if s == GOAL:\n",
        "        if s_next == s:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    # If wall hit, next state is current state\n",
        "    if hit_wall(s, a):\n",
        "        # Illegal action with s = s'\n",
        "        if s_next == s:\n",
        "            return 1\n",
        "        # Illegal action with s != s'\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    # Legal action with adjacent s and s'\n",
        "    if s_next == s_next_calc(s, a):\n",
        "        return 1 - alpha\n",
        "    else:\n",
        "        # Agent moves to another adjacent state instead of s' due to noise in a\n",
        "        # Generate all other neighbors of s by applying actions other than a\n",
        "        other_s_next = [s_next_calc(s, i)\n",
        "                    for i in range(len(A)) if i != a]\n",
        "        if s_next in other_s_next:\n",
        "            return alpha/4\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVgNRD13EUaa"
      },
      "source": [
        "## Policy Iteration\n",
        "\n",
        "In policy iteration, we define three functions:\n",
        "\n",
        "* policy_evaluation\n",
        "* policy_improvement\n",
        "* policy_iteration\n",
        "\n",
        "Refer to the lecture slides on policy iteration and value iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "1-Kv6DAFEUab",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(policy, S, Pr, alpha, discount, theta, ctr):\n",
        "\n",
        "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "    V_prev = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "\n",
        "    for _ in range(ctr):\n",
        "        # chose an initial delta value for the convergence test\n",
        "        #delta = '???'\n",
        "        delta = 0\n",
        "        V_prev = V.copy()\n",
        "        for s, _ in np.ndenumerate(S):\n",
        "\n",
        "            # action by the policy\n",
        "            #a = '???'\n",
        "            a = policy[s]\n",
        "            # update value function for the state s\n",
        "#             V[s] = '???'\n",
        "            V[s] = sum(\n",
        "                Pr(s_next, s, a, alpha) * (R(s, a) + discount * V[s_next])\n",
        "                for s_next in itertools.product(range(GRID_SIZE), repeat=2)\n",
        "            )\n",
        "            # convergece test\n",
        "            delta = max(delta, abs(V[s] - V_prev[s]))\n",
        "\n",
        "        if delta < theta: break\n",
        "\n",
        "    return V, delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "hCJav5gtEUab"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(V, S, A, Pr, alpha, discount):\n",
        "\n",
        "    policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=int)\n",
        "    policy_stable = True\n",
        "\n",
        "    for s, _ in np.ndenumerate(S):\n",
        "\n",
        "        old_action = policy[s]\n",
        "        Q = np.zeros(len(A))\n",
        "        for a in range(len(A)):\n",
        "            # update Q function at state, s, and action, a\n",
        "            # Q[a] = '???'\n",
        "            for i_prime in range(GRID_SIZE):\n",
        "                for j_prime in range(GRID_SIZE):\n",
        "                    s_prime = (i_prime, j_prime)\n",
        "                    r = R(s, a)\n",
        "                    Q[a] += Pr(s_prime, s, a, alpha) * (r + discount * V[s_prime])\n",
        "\n",
        "            # update policy at state s\n",
        "            # policy[s] = '???'\n",
        "            policy[s] = np.argmax(Q)\n",
        "\n",
        "        if old_action != policy[s]: policy_stable = False\n",
        "\n",
        "    return policy, policy_stable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "v87MzWcIEUab"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(S, A, Pr, alpha, discount, theta, n_eval, plot_enable, plot=None):\n",
        "    \"\"\"\n",
        "    :param list S: set of states\n",
        "    :param list A: set of actions\n",
        "    :param function Pr: transition function\n",
        "    :param float alpha: noise\n",
        "    :param float discount: discount factor\n",
        "    :param float theta: tolerance, which determines when to end iterations\n",
        "    :param int n_eval: number of evaluations\n",
        "    :param plot: list of iteration numbers to plot\n",
        "    \"\"\"\n",
        "\n",
        "    epsilon = 0\n",
        "\n",
        "    # For Task 4 ######\n",
        "    start_time = 0\n",
        "    end_time = 0\n",
        "    time = 0\n",
        "    total_time = 0\n",
        "    ###################\n",
        "\n",
        "    plt.ion()\n",
        "    policy = np.random.randint(0, len(A), (GRID_SIZE, GRID_SIZE))\n",
        "    count=0\n",
        "\n",
        "    while True:\n",
        "\n",
        "        start_time = datetime.datetime.now() # For Task 4\n",
        "\n",
        "        V, delta = policy_evaluation(policy, S, Pr, alpha, discount, theta, n_eval)\n",
        "        policy , policy_stable = policy_improvement(V, S, A, Pr, alpha, discount)\n",
        "\n",
        "\n",
        "        # For Task 4\n",
        "        end_time = datetime.datetime.now()\n",
        "        time = (end_time - start_time).total_seconds() # calculate time taken [seconds] for one iteration\n",
        "        total_time += time\n",
        "\n",
        "\n",
        "        # plot intermediate result\n",
        "        if (plot and count+1 in plot) and plot_enable:\n",
        "            plot_value_grid(V, policy, msg='Policy iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
        "\n",
        "        if delta == 0:\n",
        "            if plot_enable :\n",
        "                plot_value_grid(V, policy, msg='Policy iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
        "            break\n",
        "\n",
        "        if (delta > 0 and delta<=epsilon) or count==max_it or policy_stable:\n",
        "            if(count == max_it):\n",
        "                print('Policy iteration failed to converge for alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
        "            if(plot_enable):\n",
        "                 plot_value_grid(V, policy, msg='Policy iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
        "            break\n",
        "        count=count+1\n",
        "\n",
        "    return V, policy, count, total_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_feJtvmWEUab"
      },
      "source": [
        "## Value Iteration\n",
        "\n",
        "We use the following function for value iteration. See slides starting from 61."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "C74aSbLaEUab"
      },
      "outputs": [],
      "source": [
        "def value_iteration(S, A, Pr, alpha, discount, plot_enable, plot=None):\n",
        "    \"\"\"\n",
        "    :param list S: set of states\n",
        "    :param list A: set of actions\n",
        "    :param function Pr: transition function\n",
        "    :param float alpha: noise\n",
        "    :param float discount: discount factor\n",
        "    \"\"\"\n",
        "    # For Task 4 ######\n",
        "    start_time = 0\n",
        "    end_time = 0\n",
        "    time = 0\n",
        "    total_time = 0\n",
        "    ###################\n",
        "\n",
        "    plt.ion()\n",
        "\n",
        "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "    V_prev = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "    optimal_policy = np.random.randint(0, len(A), (GRID_SIZE, GRID_SIZE), dtype=int)\n",
        "\n",
        "    count=0\n",
        "    while True:\n",
        "\n",
        "        start_time = datetime.datetime.now() # For Task 4\n",
        "\n",
        "        delta = 0\n",
        "        V_prev = V.copy()\n",
        "\n",
        "        for s,_ in np.ndenumerate(S):\n",
        "\n",
        "            Q = np.zeros(len(A))\n",
        "            for a in range(len(A)):\n",
        "            # Q[a] = '???'  # expression for the Q function at state, s, and action, a\n",
        "                Q[a] = sum(\n",
        "                Pr(s_prime, s, a, alpha) * (R(s, a) + discount * V_prev[s_prime])\n",
        "                for s_prime in itertools.product(range(GRID_SIZE), repeat=2)\n",
        "            )\n",
        "            # V[s] = '???'\n",
        "                V[s] = np.max(Q)\n",
        "            # delta = max(delta, '???') # set the validation condition for the convergence\n",
        "                delta = max(delta, abs(V[s] - V_prev[s]))\n",
        "            #print(delta)\n",
        "\n",
        "            optimal_policy[s] = np.argmax(Q)\n",
        "\n",
        "        # For Task 4\n",
        "        end_time = datetime.datetime.now()\n",
        "        time = (end_time - start_time).total_seconds() # calculate time taken [seconds] for one iteration\n",
        "        total_time += time\n",
        "\n",
        "\n",
        "        # plot current value and optimal policy\n",
        "        if (plot and count+1 in plot) and plot_enable:\n",
        "            plot_value_grid(V, optimal_policy, msg='Value iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
        "            plt.pause(0.1)\n",
        "        if delta <= 0 or count==max_it:\n",
        "            if count == max_it:\n",
        "                print('Value iteration failed to converge for alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
        "            if(plot_enable):\n",
        "                plot_value_grid(V, optimal_policy, msg='Value iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
        "                plt.pause(0.1)\n",
        "            break\n",
        "        count=count+1\n",
        "    return V, optimal_policy, count, total_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vyld48sUEUab"
      },
      "source": [
        "#### We will use the following utility function to plot the grid with values from V:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Vx-euSGWEUab"
      },
      "outputs": [],
      "source": [
        "def plot_value_grid(V, policy, msg=\"\"):\n",
        "    fig = plt.figure(figsize=(5, 5), tight_layout=True)\n",
        "    plt.title(msg)\n",
        "    plt.imshow(V)\n",
        "\n",
        "    quiver_action_dict = [\n",
        "            [1, 0],\n",
        "            [-1, 0],\n",
        "            [0, -1],\n",
        "            [0, 1],\n",
        "            [0, 0]\n",
        "    ]\n",
        "    for k, a in np.ndenumerate(policy):\n",
        "        if(S[k] == 0): # do not print the arrows on walls, to increase readibility\n",
        "            q = plt.quiver(k[1], k[0], quiver_action_dict[a][1], quiver_action_dict[a][0])\n",
        "    #fig.colorbar(q)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX17TgExEUab"
      },
      "source": [
        "## Tasks\n",
        "### A. Find the optimal solution by two methods for  $\\alpha$ = 0 (no noise)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUGanpXzEUab"
      },
      "source": [
        "### I. Policy Iteration\n",
        "\n",
        "We are using iterative policy evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "LZL7XN7iEUac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "71167e96-4771-4cbc-90e4-58b823ce6f64"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-8fb18c8040bb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_pol_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m#number of policy evalutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpol2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpol_max_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_pol_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_enable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-fb95000accf7>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[0;34m(S, A, Pr, alpha, discount, theta, n_eval, plot_enable, plot)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For Task 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpolicy_stable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_improvement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-846bda72649d>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(policy, S, Pr, alpha, discount, theta, ctr)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# update value function for the state s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#             V[s] = '???'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             V[s] = sum(\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mPr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_next\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ms_next\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRID_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-846bda72649d>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#             V[s] = '???'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             V[s] = sum(\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mPr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_next\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ms_next\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRID_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-64-3cf4119506da>\u001b[0m in \u001b[0;36mPr\u001b[0;34m(s_next, s, a, alpha)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# If wall hit, next state is current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhit_wall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Illegal action with s = s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms_next\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-8e5510ee8680>\u001b[0m in \u001b[0;36mhit_wall\u001b[0;34m(curr, action)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# 3: 'right':( 0,  1),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 4: 'stay': ( 0,  0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_new\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_pol_eval = 100 #number of policy evalutions\n",
        "val2, pol2, pol_max_iter, time_taken = policy_iteration(S, A, Pr, alpha=0, discount=1, theta = 1e-6, n_eval=n_pol_eval, plot_enable = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBac_v5gEUac"
      },
      "source": [
        "### II. Value Iteration\n",
        "\n",
        "Run for 100 iterations.\n",
        "Plot the value function and the optimal policy, at iterations 1, 25, 50, 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-diPfkcFEUac",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "val1, pol1, val_max_iter, time_taken = value_iteration(S, A, Pr, alpha=0, discount=1, plot_enable=True, plot=[1, 25, 50, 100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55aV-hYiEUac"
      },
      "source": [
        "Let's visualize these results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usH6kFwPEUac"
      },
      "outputs": [],
      "source": [
        "# Plot for policy iteration alpha = 0, k_max = 100, run for 100 iterations\n",
        "plot_value_grid(val2, pol2, msg='Policy Iteration, {} iterations, {} evaluations'.format(pol_max_iter, n_pol_eval))\n",
        "# Plot for value iteration alpha = 0, 100 iterations\n",
        "plot_value_grid(val1, pol1, msg='Value Iteration, {} iterations'.format(val_max_iter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlBjcTl5maO9"
      },
      "source": [
        "### III. Run Policy Iteration and Value Iteration for  five different discounted factors $\\gamma \\in \\{0, 0.1, 0.4, 0.9, 1\\}$ and perform the following tasks\n",
        "\n",
        "1. Explain the change in utilities for different $\\gamma$\n",
        "2. Explain the change in optimal policies for different $\\gamma$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2kxB45rmv5a"
      },
      "source": [
        "#### Policy iteration for ùõæ‚àà{0,0.1,0.4,0.9,1} :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Q3Kafm5Tmv5a"
      },
      "outputs": [],
      "source": [
        "n_pol_eval = 100 #number of policy evalutions\n",
        "pol_time_list = []\n",
        "for gamma in [0,0.1,0.4,0.9,1] :\n",
        "    val2, pol2, pol_max_iter, comp_time = policy_iteration(S, A, Pr, alpha=0, discount=gamma, theta=1e-6, n_eval=n_pol_eval, plot_enable = True)\n",
        "    pol_time_list.append(comp_time)\n",
        "    plt.pause(0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DWb9OCvmv5b"
      },
      "source": [
        "#### Value iteration for ùõæ‚àà{0,0.1,0.4,0.9,1} :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "FcGv5hmfmv5b"
      },
      "outputs": [],
      "source": [
        "n_pol_eval = 100 #number of policy evalutions\n",
        "val_time_list = []\n",
        "for gamma in [0,0.1,0.4,0.9,1] :\n",
        "    val1, pol1, val_max_iter, comp_time = value_iteration(S, A, Pr, alpha=0, discount=gamma, plot_enable=True, plot=None)\n",
        "    val_time_list.append(comp_time)\n",
        "    plt.pause(0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTgGQ8mNmv5h"
      },
      "source": [
        "#### Your explanations:"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "4imVOdvHmv5h"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0vOpHommhYZ"
      },
      "source": [
        "### IV. Plot $\\gamma$ VS computational time for the given $\\gamma$ 's in Task 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID5S6aO4mv5i"
      },
      "source": [
        "#### ùõæ VS computational time for Policy iteration :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzAexUIkmv5i"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.xlabel('Discount')\n",
        "plt.ylabel('Computational time [sec]')\n",
        "plt.plot([0,0.1,0.4,0.9,1], pol_time_list)\n",
        "plt.title('Discount VS Computational time for Policy iteration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmoed2_3mv5i"
      },
      "source": [
        "#### ùõæ VS computational time for Value iteration :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvr-NV1imv5i"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.xlabel('Discount')\n",
        "plt.ylabel('Computational time [sec]')\n",
        "plt.plot([0,0.1,0.4,0.9,1], val_time_list)\n",
        "plt.title('Discount VS Computational time for Value iteration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo7GEfU8EUac"
      },
      "source": [
        "### B. Repeat Value and Policy iteration for 10, 20 and 100 iterations with $\\alpha \\in \\{0.2, 0.8\\}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGYS8OnhEUac"
      },
      "source": [
        "#### I. Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3NvJ-GhEUac",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "for idx1, alpha in enumerate(ALPHA):\n",
        "    val, pol, pol_iter, time_taken = policy_iteration(S, A, Pr, alpha=alpha, discount = 1, theta=1e-6, n_eval=100, plot_enable = True, plot=[10, 20, 100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_XEL-9LEUac"
      },
      "source": [
        "#### II. Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r-yO1YBEUac",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "for idx1, alpha in enumerate(ALPHA):\n",
        "    val, pol, val_iter, comp_time = value_iteration(S, A, Pr, alpha = alpha, discount = 1, plot_enable=True, plot=[10, 20, 100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RiVQFGDEUac"
      },
      "source": [
        "**Summarize insights and your observations in Question B-I and B-II**"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "2sLSoI6A8EAH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60btItBW7lAV"
      },
      "source": [
        "### C. Explain (up to 5 sentences) the differences between the approaches in HW1 (search, A*) and the approaches in the current assignment (MDP/Value/Policy)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "47IjkQggmv5k"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcpDOZykmv5k"
      },
      "source": [
        "## <h1><center> Part-2 </center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og97Jzqtmv5k"
      },
      "source": [
        "#### <h1><center> Utility functions to be implemented / PSEUDOCODE </center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMFLqjA4mv5l"
      },
      "source": [
        "Action space: Same as Part-1.\n",
        "<br>\n",
        "State space: Design a 10x10 maze with 3 obstacles (an obstacle is a fence with a few blocks of \"wall\"). You can use the get_maze() function from Part-1.\n",
        "<br><br>\n",
        "dS - size of the maze with dimensions: dS x dS\n",
        "<br>\n",
        "dA - number of actions\n",
        "<br>\n",
        "Goal - goal state.\n",
        "<br><br>\n",
        "\n",
        "PSEUDOCODE: function validState(s) =  returns true if (state, s, is within the maze boundaries) AND (s is NOT in the obstacles)\n",
        "\n",
        "PSEUDOCODE: function BuildMaze(dS, dA, Goal):\n",
        "<br>\n",
        "\n",
        "\t# dynamics tensor with dimensions: |dS| x |dS| x |dA| x |dS| x |dS| x 1, where the\n",
        "\t# dimensions are S‚ÇÅ, S‚ÇÇ, A, S‚ÇÅ‚Ä≤, S‚ÇÇ‚Ä≤. e.g., S‚ÇÇ is the current second coordinate of the state, and S‚ÇÅ‚Ä≤ is the first coordinate of the state at the next time step.\n",
        "\tPs‚Ä≤_sa = zeros(dS, dS, dA, dS, dS)\n",
        "\n",
        "\t# the reward tensor with the same dimension as the dynamics\n",
        "\t# reward is 0 at the Goal state, -10000 if agent hits the wall, and -1 elsewhere.\n",
        "\tRs‚Ä≤sa  = -ones(dS, dS, dA, dS, dS)\n",
        "\n",
        "\t# iterate over the valid states\n",
        "\tfor s in filter(validState, (x->x.I).(CartesianIndices((dS, dS))))\n",
        "\t\tif s ‚àà Goal\n",
        "\t\t\tPs‚Ä≤_sa[s..., :, s...] .= 1.0 # all the actions get prob 1 at the goal\n",
        "\t\t\tRs‚Ä≤sa[s..., :, s...]  .= 0.0 # all the actions get reward 0\n",
        "\t\t\tcontinue\n",
        "\t\tend\n",
        "\n",
        "\t\tfor a in Actions # the same action set at each state\n",
        "\t\t\t# if \"next state is valid\" move to it, otherwise stay at place\n",
        "\t\t\ts‚Ä≤ = validState(s .+ a[2]) ? s .+ a[2] : s\n",
        "\t\t\tPs‚Ä≤_sa[s..., a[1], s‚Ä≤...] = 1.0\n",
        "\t\tend\n",
        "\tend\n",
        "\t\"sanity test:\" forall a, s : sum_s‚Ä≤ Ps‚Ä≤_sa = 1\n",
        "\treturn Ps‚Ä≤_sa, Rs‚Ä≤sa\n",
        "end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB8BQ0admv5l"
      },
      "source": [
        "#### <h1><center> TASKS </center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh9WNTZcmv5l"
      },
      "source": [
        "### Placeholder for the definition of global variables, functions, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiZF54hbmv5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNp5umhXmv5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L0kfsM-mv5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8lPbP8fmv5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2ldnu38mv5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJP9e6CSmv5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jQIec6amv5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnfUI_EJmv5m"
      },
      "source": [
        "### Task 1 ###\n",
        "Build your maze with dimensions 10x10 and 3 fences, and the goal state (exit)\n",
        "in one of the corners of the maze. Visualize the maze layout on 2D plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_bGwzSgmv5n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2auzPs8mv5n"
      },
      "source": [
        "### Task 2 ###\n",
        "Implement the Policy Evaluation (PE) algorithm for a deterministic policy, œÄ.\n",
        "<br>\n",
        "The dimensions of the policy œÄ[a | s] are |dS| x |dS| x |dA| x 1 x 1 x 1.\n",
        "<br>\n",
        "There is a single possible action at every state.\n",
        "<br>\n",
        "E.g., œÄ[UP | some_state] = [1, 0, 0, 0, 0], see 'Actions' above.\n",
        "<br>\n",
        "E.g., œÄ[Stay | another_state] = [0, 0, 0, 0, 1], see 'Actions' above.\n",
        "<br><br>\n",
        "Evaluate a random deterministic policy, œÄ.  Plot Value of a random policy on 2D plot.\n",
        "<br><br>\n",
        "Instructions:\n",
        "<br>\n",
        "The policy is stationary, which means œÄ[a‚Ä≤ | s‚Ä≤] is permute_dimensions(œÄ[a | s], dim1->dim4, dim2->dim5, dim3->dim6)\n",
        "<br>\n",
        "Use broadcasting '.*', e.g.,\n",
        "<br>\n",
        "p(s‚Ä≤ a‚Ä≤ | s, a) = œÄ[a‚Ä≤|s‚Ä≤] .* p(s‚Ä≤ | s, a)\n",
        "<br>\n",
        "sum_s‚Ä≤p(s‚Ä≤ | s, œÄ[a|s]) .* V[s‚Ä≤], where V[s‚Ä≤] is the value of the next state with dimensions 1 x 1 x 1 x dS x dS x 1.\n",
        "<br>\n",
        "The value of the current state has dimensions dS x dS x 1 x 1 x 1 x 1.\n",
        "\"V of the next state\" is permute_dimensions(\"V of the current state\", dims1->dims4, dims2->dims5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX_p8ea6mv5n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R78XSNrOmv5n"
      },
      "source": [
        "### Task 3 ###\n",
        "Repeat Task 2 with manually setting the optimal actions in the radius of 2 states from the goal state.\n",
        "Explain your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLb4oIWAmv5o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEeIfyJBmv5o"
      },
      "source": [
        "#### Your observations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbf9JIP2mv5o"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQurM0oYmv5o"
      },
      "source": [
        "### Task 4 ###\n",
        "Implement the Policy Improvement (PI) Algorithm, and find the optimal policy œÄ*.\n",
        "Visualize the optimal value function, V_i, on a 2D plot at 3 different iterations, i, of PI.\n",
        "Explain your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQMjB1gymv5o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PehWU6Zdmv5o"
      },
      "source": [
        "#### Your observations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxBqVQfamv5p"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}